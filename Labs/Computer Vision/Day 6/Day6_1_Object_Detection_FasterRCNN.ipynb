{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "936c6c87",
      "metadata": {
        "id": "936c6c87"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824b7295",
      "metadata": {
        "papermill": {
          "duration": 0.00414,
          "end_time": "2025-02-02T15:14:30.090143",
          "exception": false,
          "start_time": "2025-02-02T15:14:30.086003",
          "status": "completed"
        },
        "tags": [],
        "id": "824b7295"
      },
      "source": [
        "# **üöÄ Object Detection with Faster R-CNN**\n",
        "In this lab, we will:\n",
        "\n",
        "‚úÖ Build a **custom Dataset class** for **Pascal VOC dataset**  \n",
        "‚úÖ Use a **pretrained Faster R-CNN model** for object detection  \n",
        "‚úÖ Train and evaluate the model  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df1282f",
      "metadata": {
        "papermill": {
          "duration": 0.003161,
          "end_time": "2025-02-02T15:14:30.096945",
          "exception": false,
          "start_time": "2025-02-02T15:14:30.093784",
          "status": "completed"
        },
        "tags": [],
        "id": "8df1282f"
      },
      "source": [
        "## **1Ô∏è‚É£ Dataset Class**\n",
        "We use the **Pascal VOC 2007 dataset**, which contains images with **bounding boxes and labels**.  \n",
        "PyTorch provides a built-in dataset loader: `torchvision.datasets.VOCDetection`."
      ]
    },
    {
      "cell_type": "code",
      "id": "4f60e0b8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T15:14:30.104558Z",
          "iopub.status.busy": "2025-02-02T15:14:30.104288Z",
          "iopub.status.idle": "2025-02-02T15:15:32.384726Z",
          "shell.execute_reply": "2025-02-02T15:15:32.383763Z"
        },
        "papermill": {
          "duration": 62.28615,
          "end_time": "2025-02-02T15:15:32.386383",
          "exception": false,
          "start_time": "2025-02-02T15:14:30.100233",
          "status": "completed"
        },
        "tags": [],
        "id": "4f60e0b8"
      },
      "source": [
        "### **üîπ Load & Transform Dataset**\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Define dataset path and transformations\n",
        "data_path = \"./VOC_data/\"\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "# Load Pascal VOC dataset (train & test)\n",
        "train_dataset = VOCDetection(root=data_path, year='2007', image_set='train', download=True, transform=transform)\n",
        "test_dataset = VOCDetection(root=data_path, year='2007', image_set='test', download=True, transform=transform)\n",
        "\n",
        "# Custom collate function to handle variable number of bounding boxes\n",
        "def custom_collate_fn(data):\n",
        "    return tuple(zip(*data))\n",
        "\n",
        "# Create DataLoaders                 (Tip: Use lower batch size if encountered Out-of-Memory (OOM) error in Training)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n",
        "\n",
        "\n",
        "#why can't we just create a function that generalises to this datatype?\n"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "id": "63daca94",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T15:15:32.425082Z",
          "iopub.status.busy": "2025-02-02T15:15:32.424677Z",
          "iopub.status.idle": "2025-02-02T15:15:32.429156Z",
          "shell.execute_reply": "2025-02-02T15:15:32.428465Z"
        },
        "papermill": {
          "duration": 0.024824,
          "end_time": "2025-02-02T15:15:32.430374",
          "exception": false,
          "start_time": "2025-02-02T15:15:32.405550",
          "status": "completed"
        },
        "tags": [],
        "id": "63daca94"
      },
      "source": [
        "# Mappings of label names (found in dataset annotation) to integer IDs (or classes) which we will feed to the model\n",
        "voc_classes = {\n",
        "    \"aeroplane\": 0,\n",
        "    \"bicycle\": 1,\n",
        "    \"bird\": 2,\n",
        "    \"boat\": 3,\n",
        "    \"bottle\": 4,\n",
        "    \"bus\": 5,\n",
        "    \"car\": 6,\n",
        "    \"cat\": 7,\n",
        "    \"chair\": 8,\n",
        "    \"cow\": 9,\n",
        "    \"diningtable\": 10,\n",
        "    \"dog\": 11,\n",
        "    \"horse\": 12,\n",
        "    \"motorbike\": 13,\n",
        "    \"person\": 14,\n",
        "    \"pottedplant\": 15,\n",
        "    \"sheep\": 16,\n",
        "    \"sofa\": 17,\n",
        "    \"train\": 18,\n",
        "    \"tvmonitor\": 19,\n",
        "}\n",
        "\n",
        "#  Reverse of label to class id mapping. needed because the model predictions will be ids and we need to change it to label to visualize it.\n",
        "reverse_voc_classes = {v: k for k, v in voc_classes.items()}\n"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "id": "a7efd317",
      "metadata": {
        "papermill": {
          "duration": 0.018153,
          "end_time": "2025-02-02T15:15:32.467163",
          "exception": false,
          "start_time": "2025-02-02T15:15:32.449010",
          "status": "completed"
        },
        "tags": [],
        "id": "a7efd317"
      },
      "source": [
        "### **üîπ Why Do We Need a Custom `collate_fn`?**\n",
        "Unlike classification datasets, where each image has a **fixed shape and label**, object detection images have **variable numbers of bounding boxes**.  \n",
        "\n",
        "- The default `collate_fn` (which applies `torch.stack`) **doesn't work**, since bounding box tensors have different shapes.  \n",
        "- Instead, we **return a tuple** that **keeps individual image-label pairs separate**.\n",
        "\n",
        "##### Before using custom collate_fn:\n",
        "```python\n",
        "data = [\n",
        "    (image1, dict1),  \n",
        "    (image2, dict2),\n",
        "    ...  \n",
        "]\n",
        "```\n",
        "##### After:\n",
        "```python\n",
        "images_tuple = (image1, image2, ...)  \n",
        "targets_tuple = (dict1, dict2, ...)   \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47cd40bf",
      "metadata": {
        "papermill": {
          "duration": 0.019458,
          "end_time": "2025-02-02T15:15:32.504979",
          "exception": false,
          "start_time": "2025-02-02T15:15:32.485521",
          "status": "completed"
        },
        "tags": [],
        "id": "47cd40bf"
      },
      "source": [
        "## **2Ô∏è‚É£ Model Class**\n",
        "We use a **pretrained Faster R-CNN with a MobileNetV3-Large backbone**.\n",
        "\n",
        "### **üîπ Modify the Model**\n",
        "- The default model is trained on **COCO dataset** with **91 classes**.\n",
        "- We modify the classifier to detect **20 Pascal VOC classes**.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "524ed7ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T15:15:32.542958Z",
          "iopub.status.busy": "2025-02-02T15:15:32.542626Z",
          "iopub.status.idle": "2025-02-02T15:15:34.148855Z",
          "shell.execute_reply": "2025-02-02T15:15:34.147973Z"
        },
        "papermill": {
          "duration": 1.627058,
          "end_time": "2025-02-02T15:15:34.150216",
          "exception": false,
          "start_time": "2025-02-02T15:15:32.523158",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "524ed7ea",
        "outputId": "bf5a0ce2-eb58-4b04-ec00-442e40dec674"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "# Load pretrained Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "\n",
        "# Change number of output classes to match Pascal VOC dataset\n",
        "num_classes = 20  # Pascal VOC has 20 object classes\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features  # Input features for predictor\n",
        "\n",
        "# Replace final layer with new predictor\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "# Freeze the backbone and just finetune the head (You can finetune the whole model, but it'd take time and resources)\n",
        "model.requires_grad_(False)\n",
        "model.roi_heads.box_predictor = model.roi_heads.box_predictor.requires_grad_(True)\n",
        "\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): Conv2dNormActivation(\n",
              "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-1): 2 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=20, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=80, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give the command to download ultralytics\n",
        "\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "9hv7V-r--UuQ"
      },
      "id": "9hv7V-r--UuQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import RTDETR\n",
        "\n",
        "# Load a COCO-pretrained RT-DETR-l model\n",
        "model = RTDETR(\"rtdetr-l.pt\")\n",
        "\n",
        "# Display model information (optional)\n",
        "model.info()\n",
        "\n",
        "# Train the model on the COCO8 example dataset for 100 epochs\n",
        "results = model.train(data=\"coco8.yaml\", epochs=20, imgsz=256)\n",
        "\n",
        "# Run inference with the RT-DETR-l model on the 'bus.jpg' image\n",
        "results = model(\"path/to/bus.jpg\")"
      ],
      "metadata": {
        "id": "TqvpCEz1-FDH"
      },
      "id": "TqvpCEz1-FDH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "def5003f",
      "metadata": {
        "papermill": {
          "duration": 0.01897,
          "end_time": "2025-02-02T15:15:34.188986",
          "exception": false,
          "start_time": "2025-02-02T15:15:34.170016",
          "status": "completed"
        },
        "tags": [],
        "id": "def5003f"
      },
      "source": [
        "## **3Ô∏è‚É£ Training and Validation Loops**\n",
        "### **üîπ Training Loop**\n",
        "- The model takes **images & targets** and computes **losses internally**. No need to define the loss.\n",
        "- We only need to **backpropagate and update the optimizer**.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "6ed05f2b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T15:15:34.228164Z",
          "iopub.status.busy": "2025-02-02T15:15:34.227920Z",
          "iopub.status.idle": "2025-02-02T15:15:34.233974Z",
          "shell.execute_reply": "2025-02-02T15:15:34.233345Z"
        },
        "papermill": {
          "duration": 0.027129,
          "end_time": "2025-02-02T15:15:34.235141",
          "exception": false,
          "start_time": "2025-02-02T15:15:34.208012",
          "status": "completed"
        },
        "tags": [],
        "id": "6ed05f2b"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, targets in tqdm(dataloader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        # Convert targets()\n",
        "        for target in targets:\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            for obj in target['annotation']['object']:\n",
        "                label = obj['name']\n",
        "                box = obj['bndbox']\n",
        "                xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                boxes.append(torch.Tensor([xmin, ymin, xmax, ymax]).to(device))\n",
        "                labels.append(voc_classes[label])\n",
        "\n",
        "            target['boxes'] = torch.stack(boxes)\n",
        "            target['labels'] = torch.Tensor(labels).type(torch.int64).to(device)\n",
        "\n",
        "        # Compute losses\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())  # Sum all losses\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "id": "4c407612",
      "metadata": {
        "papermill": {
          "duration": 0.01926,
          "end_time": "2025-02-02T15:15:34.273816",
          "exception": false,
          "start_time": "2025-02-02T15:15:34.254556",
          "status": "completed"
        },
        "tags": [],
        "id": "4c407612"
      },
      "source": [
        "### **üîπ Validation Loop**\n",
        "#### **üîπ How Do We Evaluate Object Detection Models?**\n",
        "To evaluate object detection models like **Faster R-CNN**, we need to measure **how well the predicted bounding boxes match the ground truth boxes**.\n",
        "\n",
        "![image.png](https://i.imgur.com/MDFxFMX.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìå Intersection over Union (IoU)**\n",
        "‚úÖ We consider a detection **correct** if the predicted box **overlaps significantly** with the ground truth box.  \n",
        "‚úÖ This is measured using **Intersection over Union (IoU)**, which calculates the **ratio of overlap** between the two boxes.\n",
        "\n",
        "$$\n",
        "IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
        "$$\n",
        "\n",
        "üöÄ **Higher IoU = Better Detection!**  \n",
        "\n",
        "\n",
        "![image.png](https://i.imgur.com/yNNhjwr.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìå What is mAP@0.5:0.95?**\n",
        "mAP (**mean Average Precision**) is the most commonly used **metric for object detection**.\n",
        "\n",
        "üîπ **mAP@0.5:0.95** means we compute the **average precision** at **different IoU thresholds** from **0.5 to 0.95**, increasing in steps of **0.05**.\n",
        "\n",
        "- **IoU ‚â• 0.5** ‚Üí Loose match  \n",
        "- **IoU ‚â• 0.75** ‚Üí Stricter match  \n",
        "- **IoU ‚â• 0.95** ‚Üí Extremely strict match  \n",
        "\n",
        "**mAP@0.5:0.95** takes the **average of all these values**, giving us a single number that represents how well the model performs **across different difficulty levels**.\n"
      ]
    },
    {
      "metadata": {
        "id": "5cd2ac986ab9db3c"
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install torchmetrics\n",
        "clear_output()"
      ],
      "id": "5cd2ac986ab9db3c",
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "id": "81209ee3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T15:15:34.313195Z",
          "iopub.status.busy": "2025-02-02T15:15:34.312948Z",
          "iopub.status.idle": "2025-02-02T15:15:38.090998Z",
          "shell.execute_reply": "2025-02-02T15:15:38.090302Z"
        },
        "papermill": {
          "duration": 3.799644,
          "end_time": "2025-02-02T15:15:38.092514",
          "exception": false,
          "start_time": "2025-02-02T15:15:34.292870",
          "status": "completed"
        },
        "tags": [],
        "id": "81209ee3"
      },
      "source": [
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# Initialize metric\n",
        "metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    \"\"\"Evaluates the model using mAP@0.5:0.95.\"\"\"\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images = [img.to(device) for img in images]\n",
        "            preds = model(images)\n",
        "\n",
        "            # Convert predictions to correct format\n",
        "            processed_preds = []\n",
        "            for pred in preds:\n",
        "                processed_preds.append({\n",
        "                    \"boxes\": pred[\"boxes\"].cpu(),\n",
        "                    \"scores\": pred[\"scores\"].cpu(),\n",
        "                    \"labels\": pred[\"labels\"].cpu()\n",
        "                })\n",
        "\n",
        "            # Convert ground truth targets\n",
        "            processed_targets = []\n",
        "            for target in targets:\n",
        "                gt_boxes = []\n",
        "                gt_labels = []\n",
        "                for obj in target['annotation']['object']:\n",
        "                    label = obj['name']\n",
        "                    box = obj['bndbox']\n",
        "                    xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                    gt_boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    gt_labels.append(voc_classes[label])\n",
        "\n",
        "                processed_targets.append({\n",
        "                    \"boxes\": torch.tensor(gt_boxes).cpu(),\n",
        "                    \"labels\": torch.tensor(gt_labels).cpu()\n",
        "                })\n",
        "\n",
        "            # Update metric\n",
        "            metric.update(processed_preds, processed_targets)\n",
        "\n",
        "    return metric.compute()  # Compute final mAP scores"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "id": "bc0abbe7",
      "metadata": {
        "papermill": {
          "duration": 0.01929,
          "end_time": "2025-02-02T15:15:38.131855",
          "exception": false,
          "start_time": "2025-02-02T15:15:38.112565",
          "status": "completed"
        },
        "tags": [],
        "id": "bc0abbe7"
      },
      "source": [
        "## **4Ô∏è‚É£ Running Training & Validation**"
      ]
    },
    {
      "cell_type": "code",
      "id": "35c395d8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T15:15:38.171378Z",
          "iopub.status.busy": "2025-02-02T15:15:38.170976Z",
          "iopub.status.idle": "2025-02-02T15:48:25.956955Z",
          "shell.execute_reply": "2025-02-02T15:48:25.955928Z"
        },
        "papermill": {
          "duration": 1967.807205,
          "end_time": "2025-02-02T15:48:25.958493",
          "exception": false,
          "start_time": "2025-02-02T15:15:38.151288",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "35c395d8",
        "outputId": "d9be2eae-bf1b-4af0-e882-5898f1b02095"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "num_epochs = 10  # Set number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    mAP_results = validate(model, test_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
        "    print(f\"mAP@0.5:0.95 for Test: {mAP_results['map']:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [01:32<00:00, 27.07it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4952/4952 [02:30<00:00, 32.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.9682\n",
            "mAP@0.5:0.95 for Test: 0.4480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [01:24<00:00, 29.45it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4952/4952 [02:28<00:00, 33.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 0.5869\n",
            "mAP@0.5:0.95 for Test: 0.4649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [01:24<00:00, 29.77it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4952/4952 [02:27<00:00, 33.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 0.5487\n",
            "mAP@0.5:0.95 for Test: 0.4665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [01:23<00:00, 29.78it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4952/4952 [02:27<00:00, 33.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 0.5322\n",
            "mAP@0.5:0.95 for Test: 0.4726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [01:25<00:00, 29.27it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4952/4952 [02:29<00:00, 33.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 0.5212\n",
            "mAP@0.5:0.95 for Test: 0.4727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [01:25<00:00, 29.40it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4952/4952 [02:36<00:00, 31.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 0.5112\n",
            "mAP@0.5:0.95 for Test: 0.4710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [01:25<00:00, 29.33it/s]\n",
            " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4170/4952 [02:05<00:22, 35.19it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x785f82de93a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1582, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/iostream.py\", line 488, in flush\n",
            "    if not evt.wait(self.flush_timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 629, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 331, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4173/4952 [02:06<00:23, 33.11it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-1555675912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmAP_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-579499940.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Convert predictions to correct format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_roi_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    312\u001b[0m             )\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         return _multiscale_roi_align(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mx_filtered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m_multiscale_roi_align\u001b[0;34m(x_filtered, boxes, output_size, sampling_ratio, scales, mapper)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mtracing_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mper_level_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0midx_in_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mrois_per_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_in_level\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "id": "d0a7d245",
      "metadata": {
        "papermill": {
          "duration": 0.711607,
          "end_time": "2025-02-02T15:48:27.316678",
          "exception": false,
          "start_time": "2025-02-02T15:48:26.605071",
          "status": "completed"
        },
        "tags": [],
        "id": "d0a7d245"
      },
      "source": [
        "## **5Ô∏è‚É£ Visualizing Predictions vs. Ground Truth**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "qetjqo8aClQA"
      },
      "id": "qetjqo8aClQA",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "7e1447c4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-02T15:48:30.059554Z",
          "iopub.status.busy": "2025-02-02T15:48:30.059191Z",
          "iopub.status.idle": "2025-02-02T15:48:31.705801Z",
          "shell.execute_reply": "2025-02-02T15:48:31.704571Z"
        },
        "papermill": {
          "duration": 2.40335,
          "end_time": "2025-02-02T15:48:31.754464",
          "exception": false,
          "start_time": "2025-02-02T15:48:29.351114",
          "status": "completed"
        },
        "tags": [],
        "id": "7e1447c4"
      },
      "source": [
        "# Select 3 random test images\n",
        "test_indices = [2777, 4742, 777]\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create figure with 5√ó2 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(30, 60))\n",
        "axes = axes.ravel()  # Flatten axes for easy iteration\n",
        "\n",
        "for i, idx in enumerate(test_indices):\n",
        "    test_img, test_target = test_dataset[idx]\n",
        "\n",
        "    # Extract Ground Truth Boxes & Labels\n",
        "    gt_boxes = []\n",
        "    gt_annotations = []\n",
        "    for obj in test_target['annotation']['object']:\n",
        "        box = obj['bndbox']\n",
        "        xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "        gt_boxes.append([xmin, ymin, xmax, ymax])\n",
        "        gt_annotations.append(obj['name'])\n",
        "\n",
        "    # Run Model on Test Image\n",
        "    with torch.no_grad():\n",
        "        pred = model([test_img.to(device)])\n",
        "\n",
        "    pred = pred[0]\n",
        "\n",
        "    # Extract Predictions\n",
        "    pred_boxes = pred['boxes'].cpu()\n",
        "    pred_annotations = pred['labels'].cpu()\n",
        "    pred_scores = pred['scores'].cpu()\n",
        "\n",
        "    # Apply Confidence Threshold (Only keep predictions with score ‚â• 0.8)\n",
        "    valid_mask = pred_scores >= 0.8\n",
        "    pred_annotations = pred_annotations[valid_mask]\n",
        "    pred_boxes = pred_boxes[valid_mask]\n",
        "\n",
        "    # Convert Predicted Labels from Numeric to Class Names\n",
        "    pred_annotations = [reverse_voc_classes[val.item()] for val in pred_annotations]\n",
        "\n",
        "    # Overlay GT & Predictions on Image\n",
        "    img = F.to_pil_image(test_img)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Plot Ground Truth in RED\n",
        "    for bbox, annotation in zip(gt_boxes, gt_annotations):\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 5, annotation, color='r', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    # Plot Predictions in GREEN\n",
        "    for bbox, annotation in zip(pred_boxes, pred_annotations):\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 5, annotation, color='g', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Image {idx}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "bd0f36d4",
      "metadata": {
        "id": "bd0f36d4"
      },
      "source": [
        "### Contributed by: Mohamed Eltayeb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d695b2",
      "metadata": {
        "id": "d4d695b2"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a12c26",
      "metadata": {
        "id": "a2a12c26"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2046.963277,
      "end_time": "2025-02-02T15:48:34.514608",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-02-02T15:14:27.551331",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}